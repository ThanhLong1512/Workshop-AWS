[{"uri":"https://thanhlong1512.github.io/Workshop-AWS/","title":"Building a Containerized Web Application on AWS with CI/CD Pipeline","tags":[],"description":"","content":"Building a Containerized Web Application on AWS with CI/CD Pipeline Overview This workshop guides you through building a complete containerized web application system on AWS with an automated CI/CD pipeline. Starting from application development on GitHub, we‚Äôll cover Docker containerization, deployment to AWS via load balancers, and integration of security, monitoring, and notification services.\nArchitecture Includes Frontend: React.js (containerized)\nBackend: Node.js/Express.js API (containerized)\nDatabase: MongoDB or PostgreSQL\nSecurity: KMS encryption, SNS notifications\nMonitoring: CloudWatch metrics and alerts\nObjectives Learn Container Technology: Master Docker containerization for packaging and deploying applications.\nBuild CI/CD Pipeline: Automate build, test, and deploy workflows from GitHub to AWS.\nInfrastructure Management: Configure load balancing, auto-scaling, security groups, and networking.\nSecurity \u0026amp; Monitoring: Integrate WAF, KMS encryption, CloudWatch, and SNS alerts.\nProduction-Ready: Deploy a highly available, scalable, and secure system.\nEstimated Costs AWS Free Tier: EC2 t2.micro, ALB (750 hours/month)\nCloudFront: $0.085/GB for the first 10TB\nRoute 53: $0.50/hosted zone/month\nKMS: $1/key/month + $0.03/10,000 requests\nSNS: $0.50/1 million requests\nTotal Estimate: $10‚Äì30/month (varies by traffic)\nWorkshop Steps Introduction Prerequisites Application Containerization Security \u0026amp; Monitoring Application Cleanup Resource "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/3-containerization/3.3-deloy-scripts/3.3.1-deploy-fe/","title":"Create scripts to deploy React.js applications","tags":[],"description":"","content":"Introduction Deploying a React.js application to a production server is a complex process with multiple steps that require precise execution. Instead of performing these steps manually‚Äîwhich can lead to errors and wasted time‚Äîwe\u0026rsquo;ll create automated scripts to handle the entire process. Deployment scripts ensure consistency, minimize human error, and enable fast, reliable deployments. This is especially important when deploying multiple times or across different environments.\nIn a real-world production scenario, a standard React.js deployment process includes: server environment setup, dependency installation, web server configuration (Nginx), SSL setup, process management, and monitoring. Each step has its own technical details and must be executed in the correct order.\nBenefits of Scripted Deployment Consistency: Each deployment follows the exact same steps, eliminating discrepancies between deployments and ensuring a stable production environment. Time Savings: Automating repetitive tasks reduces deployment time from hours to minutes. Error Reduction: Eliminates human errors that often occur during complex manual processes. Rollback Capability: Quickly revert to a previous version if issues arise. Reusability: Scripts can be customized and reused across multiple projects and environment Implementation Steps Step 1: Go to EC2 -\u0026gt; Instances .\nStep 2: Select instances ƒë√£ t·∫°o -\u0026gt; Connect -\u0026gt; Connect\nStep 3: Run the following command to install and configure Docker: sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install docker.io -y \u0026amp;\u0026amp; sudo systemctl start docker \u0026amp;\u0026amp; sudo chmod 666 /var/run/docker.sock \u0026amp;\u0026amp; sudo systemctl enable docker \u0026amp;\u0026amp; docker \u0026ndash;version and then run docker ps to check configuration Step 4: Navigate to your GitHub repository AWS-FE, Click Settings -\u0026gt; Runner -\u0026gt; New self-hosted runner\nStep 5: Select Runner image: Linux v√† Copy and run the provided commands on your EC2 Instances\nStep 6: Start the runner service:\nec2 sudo ./svc.sh install sudo ./svc.sh start Step 7: Verify Runner Status, ensure the runner is active in GitHub: Step 8: Access Visual Studio Code then open your project name is AWS-FE Step 9: Push code to GitHub. The deployment will trigger automatically under Actions. Step 10: Monitor the deployment logs for errors Step 11: Access the application via Public IP EC2:5173 "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/2-prerequiste/2.1-createec2/2.1.1-createvpc/","title":"Create VPC","tags":[],"description":"","content":"Create VPC Lab VPC Go to VPC service management console Click Your VPC. Click Create VPC. At the Create VPC page. In the Name tag field, enter Mern-stack VPC. In the IPv4 CIDR field, enter: 10.0.0.0/24. Click Create VPC. Check VPC status after creation "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/2-prerequiste/2.3-sgandnetwork/2.3.1-createsg/","title":"Creating Security Groups","tags":[],"description":"","content":"Creating Security Groups in Amazon VPC Overview\nSecurity Groups act as virtual firewalls for AWS resources Function as instance-level access control lists (ACLs) Enable control of inbound/outbound traffic by port and protocol Creating ALB Security Group\nNavigate to the VPC console\nSelect Security Groups from the left menu Click Create security group Configure basic information Security Group name: Enter alb-security-group Description: Security group for Application Load Balancer VPC: Select Mern-stack VPC Set up Inbound/Outbound Rules Click Add rule Rule 1: Type: HTTP Source: Anywhere (allow ping from all locations) Rule 2: Type: HTTPs Source: Anywhere (allow ping from all locations) Confirm Inbound/Outbound Rules and create the Security Group Verify the created Security Group Creating Application Security Group\nNavigate to the VPC console\nSelect Security Groups from the left menu Click Create security group Configure basic information Security Group name: Enter app-security-group Description: Security group for application containers VPC: Select Mern-stack VPC Set up Inbound/Outbound Rules\nClick Add rule Rule 1: Type: HTTP Source: Anywhere (allow ping from all locations) Rule 2: Type: HTTPs Source: Anywhere (allow ping from all locations) Confirm Inbound/Outbound Rules and create the Security Group\nVerify the created Security Group "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/4-securitymonitoring/4.1-aws-kms/","title":"Data encryption with AWS KMS ","tags":[],"description":"","content":"Introduction AWS Key Management Service (KMS) provides centralized encryption key creation and management, enabling data encryption both at rest and in transit. When combined with AWS Certificate Manager (ACM), the system can automatically manage the lifecycle of SSL/TLS certificates, including issuance, renewal, and deployment across AWS services.\nFor web applications deployed on EC2 with MongoDB Atlas databases, implementing KMS encryption ensures that sensitive information such as connection strings, user credentials, payment data, and personal information is protected at an enterprise level. This is particularly critical for applications handling personal data under regulations like GDPR, CCPA, or Vietnam\u0026rsquo;s Cybersecurity Law.\nBy integrating IAM roles and policies, EC2 instances can securely access KMS keys without hard-coding credentials, adhering to the principle of least privilege and a zero-trust security model. Combined with CloudFront distributions using ACM certificates, all traffic from end-users to the application is encrypted end-to-end, creating a comprehensive security perimeter for the system.\nImplementation Steps Step 1: Navigate to AWS Console -\u0026gt; AWS KMS -\u0026gt; Create Key Step 2: Select Symmetric and Key Usage: Encrypt and decrypt Step 3: Choose Role_ec2 -\u0026gt; Next Step 4: Verify the role\u0026rsquo;s policy is correctly configured Step 5: Click Finish to create the key Step 6: Open Visual Studio Code -\u0026gt; AWS-BE Step 7: Open Terminal -\u0026gt; Run npm install aws-sdk Step 8: Create kms.js in the utils folder with the following content::\nkms.js const AWS = require(\"aws-sdk\"); AWS.config.update({ region: \"ap-southeast-1\" }); const kms = new AWS.KMS(); class KMSService { constructor() { this.keyAlias = \"alias/KMS-KEY\"; } async encrypt(plaintext) { try { console.log(\"Encrypting data...\"); const params = { KeyId: this.keyAlias, Plaintext: Buffer.from(plaintext, \"utf8\") }; const result = await kms.encrypt(params).promise(); const encrypted = result.CiphertextBlob.toString(\"base64\"); console.log(\"Data encrypted successfully\"); return encrypted; } catch (error) { console.error(\"Encryption failed:\", error.message); throw error; } } async decrypt(encryptedData) { try { console.log(\"Decrypting data...\"); const params = { CiphertextBlob: Buffer.from(encryptedData, \"base64\") }; const result = await kms.decrypt(params).promise(); const decrypted = result.Plaintext.toString(\"utf8\"); console.log(\"Data decrypted successfully\"); return decrypted; } catch (error) { console.error(\"Decryption failed:\", error.message); throw error; } } async encryptJSON(object) { const jsonString = JSON.stringify(object); return await this.encrypt(jsonString); } async decryptJSON(encryptedData) { const decryptedString = await this.decrypt(encryptedData); return JSON.parse(decryptedString); } } module.exports = new KMSService(); Step 9: Implement the four methods below in models/AccountModel.js\nStep 10: Call the encryption/decryption methods as shown below for AccountModel.js\n"},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/3-containerization/3.1-development-env/3.1.1-install-docker/","title":"Install Docker Desktop","tags":[],"description":"","content":"Overview of Docker Docker is a platform for developers and sysadmins to develop, deploy, and run applications using containers. It allows the creation of isolated and independent environments to launch and develop applications, and these environments are called containers. When you need to deploy to any server, simply run the Docker container, and your application will launch immediately.\nKey Components of Docker\nHow Docker Works\nWhat is Containerization?\nContainerization is a virtualization technology that solves this problem by packaging an application and all its dependencies (such as libraries and configurations) into an independent unit called a container. This container ensures that the software runs stably and consistently in any environment, whether on a developer\u0026rsquo;s personal computer, a test server, or a production environment in the cloud.\nBefore containers: A developer writes an application on their personal computer using Python version 3.6. After successful testing, they deploy the application to a production server, but this server is running Python version 3.7. This minor difference causes errors because some libraries are incompatible with Python 3.7.\nWith containers: The developer can package their application along with all the required libraries, including Python 3.6, into a container. When this container is deployed in any environment (local, test, production), the application will run exactly as it did during testing on the personal computer.\nThe Role of Containerization\nConsistency: Ensures that the application packaged in a container behaves the same way in every environment, whether on a developer\u0026rsquo;s machine, a test server, or a production environment.\nPortability: Containers are lightweight software packages that can be easily moved from one system to another without compatibility issues.\nScalability: Containers can be quickly started or stopped, allowing applications to scale up or down based on actual demand.\nEfficiency: Containers share the host operating system\u0026rsquo;s kernel, unlike traditional virtual machines that require a full operating system. This makes containers more resource-efficient, faster to start, and easier to manage.\nIsolation: Containers ensure that applications are isolated from each other, enhancing security. If one application is compromised, the issue does not affect other applications.\nGuide to Installing and Using Docker This guide is only for Windows operating systems.\nIf you are using macOS or another operating system, please look for the appropriate documentation, as installation steps and system requirements may differ significantly.\nVerify your operating system before proceeding to avoid errors or incompatible installations.\nDownload the Docker Desktop application from this link, Note: If your computer uses an ARM chip, download the version below. If you use Intel or AMD x86/x64 chips, download the version above. Double-click the Docker Desktop Installer.exe file downloaded in step 1 to begin installation. By default, Docker Desktop is installed at C:\\Program Files\\Docker\\Docker. Then click OK to proceed. After installation is complete, click Close and Restart. Open CMD and type wsl. If the output resembles the image below, you have succeeded. "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/1-introduce/","title":"Introduction","tags":[],"description":"","content":"Overview This architecture describes a Serverless Web Application built entirely on AWS, leveraging managed services to create a highly scalable, cost-optimized, and secure application. The application follows a 3-tier architecture model, consisting of frontend, backend, and supporting services.\nKey Components of the Architecture User Access \u0026amp; Code Management GitHub: Source code management and version control for the entire application User: End-users accessing the application via web browsers Security \u0026amp; Network Layer (L·ªõp b·∫£o m·∫≠t v√† m·∫°ng) AWS Route 53 is a highly scalable and reliable DNS (Domain Name System) service. It routes user requests to AWS infrastructure, such as EC2 instances, Elastic Load Balancers, or Amazon S3 buckets. Route 53 can also direct traffic to non-AWS infrastructure. AWS Application Load Balancer (ALB) operates at Layer 7 (Application Layer) of the OSI model, routing traffic based on request content. ALB supports path-based and host-based routing, distributing traffic across multiple target groups. It integrates seamlessly with AWS services like Auto Scaling, ECS, and Lambda. AWS CloudFront is a global Content Delivery Network (CDN) that delivers content with low latency and high transfer speeds. It caches content at edge locations worldwide, reducing page load times and improving user experience. AWS WAF (Web Application Firewall) protects applications from common web exploits like SQL injection and cross-site scripting (XSS). It allows custom rules to filter, monitor, and block malicious traffic based on IP addresses, HTTP headers, HTTP body, or URI strings. Application Layer (L·ªõp ·ª©ng d·ª•ng) Frontend Container Runs on port 80 Uses Nginx as the web server Can host React, Angular, or Vue.js applications Port 4375 for internal communications Backend Container Runs on port 4000 Uses Express.js (Node.js) framework Handles business logic \u0026amp; API endpoints Connects with other AWS services AWS Managed Services Amazon EC2 (Elastic Compute Cloud) provides scalable compute capacity in the cloud. Users can launch virtual servers (instances) with customizable CPU, memory, storage, and networking configurations. EC2 offers various instance types optimized for different workloads (compute-optimized, memory-optimized, storage-optimized). AWS KMS (Key Management Service) is a fully managed encryption key service for creating and controlling cryptographic keys. It integrates with most AWS services to protect stored data and uses Hardware Security Modules (HSMs) for enhanced security. Amazon CloudWatch is a monitoring and observability service for DevOps engineers, developers, SREs, and IT managers. It provides actionable insights to monitor applications, track system-wide performance changes, optimize resource usage, and maintain operational health.. Amazon S3 (Simple Storage Service) is a highly durable object storage service for storing and retrieving any amount of data. It offers 99.999999999% (11 nines) durability and powers millions of applications worldwide. S3 includes management features for data organization and fine-grained access control. Amazon SNS (Simple Notification Service) is a fully managed messaging service for application-to-application (A2A) and application-to-person (A2P) communication. It enables high-throughput, push-based, many-to-many messaging for distributed systems, microservices, and serverless applications. Database \u0026amp; Data Processing MongoDB: NoSQL database for application data storage Integrated with the AWS ecosystem via managed services "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/3-containerization/3.1-development-env/","title":"Prepare the development environment","tags":[],"description":"","content":"Content Install Docker Desktop Setup Git \u0026amp; Github repository Instructions for cloning the FE\u0026amp;BE project "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/2-prerequiste/2.1-createec2/","title":"Preparing VPC and EC2","tags":[],"description":"","content":"In this step, we will need to create a VPC with 2 public / private subnets. Then create 1 EC2 Instance Linux located in the public subnet, 1 EC2 Instance Windows located in the private subnet.\nTo learn how to create EC2 instances and VPCs with public/private subnets, you can refer to the lab:\nAbout Amazon EC2 Works with Amazon VPC Content Create VPC Create Subnet Create Linux server "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/3-containerization/3.2-create-dockercontainer/3.2.1-preparation/","title":"Setting Up the Deployment Environment","tags":[],"description":"","content":"Introduction Before deploying the application to production, we need to prepare the necessary environment and tools. Properly setting up the environment will ensure a smooth deployment process and help avoid unexpected errors.\nWhat We Will Prepare Required Extensions Containerization Tools CI/CD Pipeline Creating Repositories on Docker Hub Steps to Implement Install Required Extensions Step 1: Open Visual Studio Code and press Ctrl+Shift+X.\nStep 2: Search for Docker and click Install.\nStep 3: Repeat Step 2 for GitHub Actions.\nStep 4: Verify the installed extensions as shown below: Containerization Tools: Step 1: After successfully completing Section 3.1.1, open Docker Desktop.\nStep 2: Navigate as shown in the image below: Step 3: Select Personal access token \u0026ndash;\u0026gt; Generate new token. Step 4: Fill in the Docker Token Info and click Generate. Step 5: Save the two pieces of information shown below for the next steps: CI/CD Pipeline:\nStep 1: Log in to your GitHub account. Step 2: Go to your repository named AWS-FE Step 3: Navigate to Settings -\u0026gt; Secrets and variables -\u0026gt; Action Step 4: Click New repository secret Step 5: Name: DOCKER_USERNAME, Secret: thanhlep Step 6: Repeat for DOCKER_PASSWORD Create Repositories on Docker Hub\nStep 1: Log in to your Docker Hub Account Step 2: Select Repositories -\u0026gt; Create a repository Step 3: Repository name: reactjs-web v√† Visibility: Public Step 4: Repeat the same for nodejs-web "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/3-containerization/3.2-create-dockercontainer/","title":"Create Docker Containers","tags":[],"description":"","content":" Introduction to Docker and Containerization? Docker is a containerization platform that packages applications and their dependencies into lightweight, portable, and isolated containers. Containers run consistently across any environment.\nWhy Use Docker?\nConsistency: Ensures applications run identically across different environments. Portability: Easy deployment from development to production. Efficiency: More resource-efficient than virtual machines. Scalability: Effortless service scaling. Chu·∫©n b·ªã:\nB∆∞·ªõc 1: Verify Docker installation by copying this command to your bash terminal: bash docker --version B∆∞·ªõc 2: If Docker isn\u0026rsquo;t installed, follow this installation guide video B∆∞·ªõc 3: Successful installation will display output similar to the image below: N√¥Ã£i dung: Build and Test Containers Locally Create Dockerfile for Frontend (React.js) Create Dockerfile for Backend (Node.js/Express) Set up a CI/CD Pipeline for React.js and Nodejs with GitHub Actions "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/3-containerization/3.2-create-dockercontainer/3.2.2-dockerfile-fe/","title":"Create Dockerfile for Frontend (React.js)","tags":[],"description":"","content":"Introduction After setting up the development environment with Docker Desktop, Git, and other necessary tools, it\u0026rsquo;s time to create a Dockerfile to containerize the React.js application. This ensures the application runs consistently across all environments, from development to production..\nProject Directory Structure Before writing the Dockerfile, ensure your React project structure looks like this: Create a .dockerignore File\nFirst, create a .dockerignore file to exclude unnecessary files: dockerignore node_modules npm-debug.log build .dockerignore Dockerfile Dockerfile.prod .git .gitignore README.md .env .nyc_output coverage .cache .parcel-cache\" Dockerfile for Development Environment dockerfile FROM node:alpine3.18 as build WORKDIR /app COPY package.json . RUN npm install COPY . . RUN npm run build FROM nginx:1.23-alpine WORKDIR /usr/share/nginx/html RUN rm -rf ./\\* COPY --from=build /app/build . EXPOSE 80 CMD [\"nginx\", \"-g\", \"daemon off;\"] Optimized Dockerfile for Development: dockerfile # Use Node.js Alpine for smaller image size FROM node:18-alpine # Install dumb-init for proper signal handling RUN apk add --no-cache dumb-init # Create a non-root user for security RUN addgroup -g 1001 -S nodejs RUN adduser -S reactjs -u 1001 # Set working directory WORKDIR /app # Copy package files v·ªõi ownership cho user COPY --chown=reactjs:nodejs package\\*.json ./ # Switch to non-root user USER reactjs # Install dependencies RUN npm ci --only=production \u0026\u0026 npm cache clean --force # Copy source code COPY --chown=reactjs:nodejs . . # Expose port EXPOSE 3000 # Use dumb-init ƒë·ªÉ x·ª≠ l√Ω signals properly ENTRYPOINT [\"dumb-init\", \"--\"] # Start development server CMD [\"npm\", \"start\"] "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/2-prerequiste/2.2-create-policies/","title":"Create IAM Policies","tags":[],"description":"","content":"Understanding IAM Policies ‚ÑπÔ∏è What is an IAM Policy? An IAM Policy is a JSON document that defines permissions and access controls for AWS resources. These policies:\nDefine precise permissions for AWS services and resources Can be attached to IAM Users, Groups, and Roles Follow the principle of least privilege Enable fine-grained access control Policy Assignment ‚ÑπÔ∏è How Policies Work IAM Policies provide the authorization framework by:\nDefining allowed and denied actions Specifying resource-level permissions Supporting both AWS managed and customer managed policies Enabling conditional access based on tags, time, or location üîí Security Best Practices When working with IAM Policies:\nUse AWS managed policies when possible Create custom policies only when necessary Regularly review and audit policy assignments Implement least privilege access Steps to create policies Step 1: Go to AWS Console -\u0026gt; IAM\nStep 2: Select Policies -\u0026gt; Create Policies Step 3: Choose Policy editor: JSON and replace with the following JSON: json { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Sid\": \"KMSPermissions\", \"Effect\": \"Allow\", \"Action\": [ \"kms:Encrypt\", \"kms:Decrypt\", \"kms:ReEncrypt*\", \"kms:GenerateDataKey*\", \"kms:DescribeKey\" ], \"Resource\": \"*\", \"Condition\": { \"StringEquals\": { \"kms:ViaService\": \"s3.ap-southeast-1.amazonaws.com\" } } } ] } Step 4: Enter Policy Name: WebsiteKMSPolicy ‚Üí Click Create Policy Policies Step 5: Verify the created policy Policies\n"},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/3-containerization/3.3-deloy-scripts/3.3.2-deploy-be/","title":"Create scripts to deploy Node.js applications","tags":[],"description":"","content":"Introduction Deploying a Node.js application to a production server is a complex process with multiple steps that require precise execution. Instead of performing these steps manually‚Äîwhich can lead to errors and wasted time‚Äîwe\u0026rsquo;ll create automated scripts to handle the entire process. Deployment scripts ensure consistency, minimize human error, and enable fast, reliable deployments. This is especially important when deploying multiple times or across different environments.\nIn a real-world production scenario, a standard React.js deployment process includes: server environment setup, dependency installation, web server configuration (Nginx), SSL setup, process management, and monitoring. Each step has its own technical details and must be executed in the correct order.\nBenefits of Scripted Deployment Consistency: Each deployment follows the exact same steps, eliminating discrepancies between deployments and ensuring a stable production environment. Time Savings: Automating repetitive tasks reduces deployment time from hours to minutes. Error Reduction: Eliminates human errors that often occur during complex manual processes. Rollback Capability: Quickly revert to a previous version if issues arise. Reusability: Scripts can be customized and reused across multiple projects and environment Quy tr√¨nh th·ª±c hi·ªán Step 1: Go to EC2 -\u0026gt; Instances .\nStep 2: Select instances ƒë√£ t·∫°o -\u0026gt; Connect -\u0026gt; Connect\nStep 3: Run the following command to install and configure Docker: sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install docker.io -y \u0026amp;\u0026amp; sudo systemctl start docker \u0026amp;\u0026amp; sudo chmod 666 /var/run/docker.sock \u0026amp;\u0026amp; sudo systemctl enable docker \u0026amp;\u0026amp; docker \u0026ndash;version and then run docker ps to check configuration Step 4: Navigate to your GitHub repository AWS-BE, Click Settings -\u0026gt; Runner -\u0026gt; New self-hosted runner\nStep 5: Select Runner image: Linux v√† Copy and run the provided commands on your EC2 Instances\nStep 6: Start the runner service:\nec2 sudo ./svc.sh install sudo ./svc.sh start Step 7: Verify Runner Status, ensure the runner is active in GitHub: Step 8: Access Visual Studio Code then open your project name is AWS-BE Step 9: Push code to GitHub. The deployment will trigger automatically under Actions Step 10: Monitor the deployment logs for errors Step 11: Access the application via Public IP EC2:8080 "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/2-prerequiste/2.1-createec2/2.1.2-createsubnet/","title":"Create Subnet","tags":[],"description":"","content":"Create Subnet Click Subnets. Click Create subnet. At the Create subnet page. In the VPC ID section, click Mern-stack VPC. In the Subnet name field, enter Mern-stack Subnet. In the Availability Zone section, select the first Availability zone. In the field IPv4 CIRD block enter 10.0.0.0/28. 3. Scroll to the bottom of the page, click Create subnet.\nClick Mern-stack Subnet. Click Actions. Click Edit subnet settings. 5. Click Enable auto-assign public IPv4 address.\nClick Save. "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/3-containerization/3.1-development-env/3.1.2-setup-git/","title":"Install Git and Github Repository","tags":[],"description":"","content":"Overview of Git and GitHub Repository GitHub is a specialized social network for developers, serving as a project management and source code hosting platform that enables collaboration in software projects. Additionally, GitHub is a well-known service that provides Git-based source code repositories for software projects. It includes all the features of Git while also adding social features to facilitate interaction among developers.\nGit is a Distributed Version Control System (DVCS) and one of the most popular version control systems today. Git provides each developer with their own repository containing the full history of changes.\nDifference Between Git and GitHub\nGit is an open-source version control system widely used for projects of all sizes.\nGitHub is a Git server where people share and collaborate on their source code. GitHub simplifies Git usage without requiring command-line expertise.\nWhy Use GitHub?\nPopularity: Among Git hosting services, GitHub is the most widely used and preferred platform, with 4 million organizations and over 100 million developers. It offers free source code hosting, distributed version control, project management, and integration with popular CI/CD platforms like Travis CI and Jenkins. Large Community: GitHub has a strong and active developer community, making it an excellent place to explore and contribute to open-source projects. -Security: GitHub provides features like code scanning and dependency analysis to enhance source code security. Code Hosting: GitHub offers a platform to store your source code, making it easily accessible to the global developer community. Collaboration: GitHub facilitates collaboration by allowing multiple developers to contribute to the same project simultaneously. How to Use GitHub\nCreate a GitHub Account If you don‚Äôt have an account, visit GitHub\u0026rsquo;s Homepage and sign up for a free account. After entering your email and password, GitHub will send a verification email. Open the email and click the verification link to confirm your email address. Once verified, you can explore GitHub by browsing public repositories and projects to understand available features.\nInstall and Configure Git If Git is not installed on your machine, follow the Git installation guide and choose the method suitable for your operating system. To check your Git version, run: Working with Repositories\nCreating a New Repository When creating a new repository, ensure the name is unique within your repository list and add a description. By default, GitHub creates a public repository, but you can change its visibility before or after creation. Now, create two repositories named:AWS-FE v√† AWS-BE\nAfter creation, go to: Home -\u0026gt; Avatar s√°t b√™n tay ph·∫£i -\u0026gt; Your Repositories If both repositories appear, congratulations‚Äîyou‚Äôve successfully created them! "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/2-prerequiste/","title":"Preparation ","tags":[],"description":"","content":" You need to prepare the MERN stack application and github accounts to perform this containerized web application deployment lab.\nTo learn how to create the foundational AWS services, you can refer to these labs:\nAbout Amazon EC2 Works with Amazon VPC Content Prepare VPC and EC2 Create IAM Policies Security Groups \u0026amp; Network setup Create IAM Role "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/4-securitymonitoring/4.2-aws-waf/","title":"Protect web applications with AWS WAF","tags":[],"description":"","content":"Introduction AWS WAF (Web Application Firewall) is a powerful cloud security service designed to protect your web applications from common cyberattacks and malicious traffic. Operating at the application layer (Layer 7 of the OSI model), AWS WAF provides granular control over HTTP/HTTPS traffic to your application.\nWhy Use AWS WAF? In today\u0026rsquo;s increasingly complex cybersecurity landscape, web applications face various types of attacks:\nSQL Injection: Attackers inject malicious SQL code into database queries\nCross-Site Scripting (XSS): Injecting malicious JavaScript into web pages\nDDoS Attacks: Overwhelming servers with massive request volumes\nBot Traffic: Malicious bots scraping data or generating fake traffic\nGeographic Attacks: Attacks originating from high-risk regions\nAWS WAF acts as a protective shield, filtering and controlling every request before it reaches your application.\nImplementation Steps Step 1: Navigate to AWS Console -\u0026gt; EC2 -\u0026gt; Target groups-\u0026gt; Create target group Step 2: Configure the following settings:\nTarget group name: Mernstack-TG VPC: Mernstack-VPC Register target: Mernstack-EC2 Step 3: After completing the configuration, select Create target group Step 4: Navigate to AWS Console -\u0026gt; EC2 -\u0026gt; Load Balancer -\u0026gt; Create load balancer Step 5: Configure the following settings:\nLoad balancer name: Mernstack-ALB Scheme: Internet-facing IP address type: IPv4 VPC: Mern-stack VPC Availability Zones: Mern-stack Subnet v√† Mern-stack Subnet 02 Security group: alb-security-group Default Action: Mernstack-TG Optimize with service integration:WAF Step 6: After completing the configuration, select Create load balancer Step 7: Navigate to AWS Console -\u0026gt; WAF \u0026amp; Shield Step 8: Select Create web ACL Step 9: Configure the following settings:\nRegion: Asia Pacific (Singapore) Name: Mernstack-WAF Resources: Ch·ªçn Application -\u0026gt; Mernstack-ALB Rule: Ch·ªçn Add Rule -\u0026gt; Add managed rule groups Step 10 Ti·∫øn h√†nh t·∫°o Create web ACL "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/2-prerequiste/2.3-sgandnetwork/2.3.2-setupnetwork/","title":"Setup Network","tags":[],"description":"","content":"Creating NAT Gateway Overview\nNAT Gateway allows instances in private subnets to connect to the internet Ensures one-way connectivity from inside to outside, enhancing security Requires Elastic IP and must be placed in a public subnet NAT Gateway Deployment Access VPC Dashboard Select NAT Gateways Choose Create NAT gateway Configure NAT Gateway Name: Enter mernstack-gateway-public Subnet: Select Mern-stack Subnet Connectivity type: Select Public Elastic IP allocation ID: Select Elastic IP v·ª´a t·∫°o Select Create NAT gateway Confirm successful NAT Gateway creation Creating Internet Gateway Overview\nInternet Gateway (IGW) is a VPC component that enables internet connectivity Provides connection point between VPC and the internet Supports bidirectional communication for resources within VPC Internet Gateway Deployment Access VPC Dashboard Select Internet Gateways from the left menu Click Create internet gateway Configure Internet Gateway In Name tag, enter mernstack-igw Click Create internet gateway Confirm successful Internet Gateway creation VPC Connection\nAttach Internet Gateway to VPC Click Actions Select Attach to VPC Select Mern-stack VPC t·ª´ danh s√°ch (VPC ID s·∫Ω t·ª± ƒë·ªông ƒëi·ªÅn) Click Attach internet gateway After successful attachment: Creating Route Table Overview\nRoute Table is a component that routes network traffic within VPC Determines the path for packets between subnets and the internet Allows control of data flow in/out of VPC Route Table Deployment Access VPC Dashboard\nSelect Route Tables from the left menu Click Create route table Configure Route Table\nName: Enter mernstack-route-tables VPC: Select Mern-stack VPC Select Create route table Confirm successful Route Table creation Routing Configuration\nAdd route for Internet Gateway Select Actions Select Edit routes Configure new route Click Add route Destination: Enter 0.0.0.0/0 (represents the internet) Target: Select mernstack-igw and choose the created IGW Target: Select mernstack-gateway-public and choose the created NAT Gateway Click Save changes Subnet Association\nSet up subnet associations Select Subnet associations tab Click Edit subnet associations Select subnets Choose Subnet: Mern-stack Subnet Click Save associations Confirm successful subnet associations configuration "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/3-containerization/3.2-create-dockercontainer/3.2.3-dockerfile-be/","title":"Create Dockerfile for Backend (Node.js/Express)","tags":[],"description":"","content":"Introduction After setting up the development environment with Docker Desktop, Git, and other necessary tools, and completing the Dockerfile for AWS-FE, we\u0026rsquo;ll now proceed to create one for AWS-BE. This ensures our application runs consistently across all environments from development to production.\nProject Directory Structure Before writing the Dockerfile, ensure your Node.js/Express project structure looks like this: Create .dockerignore File\nFirst, create a .dockerignore file to exclude unnecessary files: dockerignore node_modules npm-debug.log .git .gitignore README.md .env .env.local .env.development .env.test .env.production .nyc_output coverage .cache logs \\*.log .DS_Store Dockerfile .dockerignore docker-compose.yml Dockerfile for Development Environment dockerfile FROM node:alpine3.18 WORKDIR /app COPY package.json ./ RUN npm install COPY . . EXPOSE 4000 CMD [ \"npm\", \"run\", \"start\" ] Optimized Dockerfile for Development: dockerfile # Use Node.js Alpine for smaller image size FROM node:18-alpine # Install dumb-init ƒë·ªÉ x·ª≠ l√Ω signals properly RUN apk add --no-cache dumb-init # Create non-root user for security RUN addgroup -g 1001 -S nodejs RUN adduser -S expressjs -u 1001 # Set working directory WORKDIR /app # Copy package files with proper ownership COPY --chown=expressjs:nodejs package\\*.json ./ # Switch to non-root user for dependency installation USER expressjs # Install dependencies (using npm ci for faster, reliable builds) RUN npm ci --only=production \u0026\u0026 npm cache clean --force # Copy source code with proper ownership COPY --chown=expressjs:nodejs . . # Expose backend port EXPOSE 4000 # Use dumb-init for proper signal handling ENTRYPOINT [\"dumb-init\", \"--\"] # Start development server CMD [\"npm\", \"run\", \"dev\"] "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/2-prerequiste/2.1-createec2/2.1.3-createec2linux/","title":"Create EC2 instance","tags":[],"description":"","content":" Go to EC2 service management console Click Instances. Click Launch instances. Name: Enter Mern-stack EC2.\nChoose an Amazon Machine Image (AMI):\nSelect Quick Start. Choose Ubuntu. AMI: Select Ubuntu Server 24.04 LTS. Key Pair Configuration:\nClick Create key pair. Fill in the details as shown in the image. Security Note: The key pair is essential for authenticating access to your EC2 instance. Store the .pem file securely and avoid sharing it. Losing the key pair may result in permanent loss of access to your instance.\nNetwork Configuration\nUnder Network, select Mern-stack VPC. Under Subnet, choose Mern-stack Subnet. Under Auto-assign Public IP, select Use subnet setting (Enable). Review the settings and click Launch instance.\nOnce the instance is launched, click View all instances to check its details Wait for about 5 minutes until the Status check shows 2/2 checks passed and the instance state changes to Running.\nPro Tip: 2/2 checks passed status confirms that both the system check (performed by AWS) and the instance check (performed by the OS) were successful. This ensures your instance is ready for use.\n"},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/3-containerization/3.3-deloy-scripts/","title":"Deploy Scripts","tags":[],"description":"","content":"Overview After completing application containerization with Docker and setting up CI/CD pipelines with GitHub Actions, the next step is to create deployment scripts to automate the process of deploying applications across different environments.\nDeploy scripts are automation scripts that simplify the deployment process, from pulling Docker images, stopping/starting containers, to performing health checks and rollbacks when necessary. This becomes particularly important when you need to deploy frequently or manage multiple environments (development, staging, production)..\nBenefits of Deploy Scripts Full Automation: Minimizes manual steps and human errors in the deployment process Consistency: Ensures deployments are executed uniformly across all environments Quick Rollback: Enables rapid reversion to previous versions when issues arise Environment Management: Handles different environment variables and configurations Health Checking: Automatically verifies application status post-deployment Deploy Scripts Architecture In this section, we\u0026rsquo;ll create deployment scripts for both Frontend (React.js) and Backend (Node.js) with the following features:\nEnvironment Detection: Automatically detects and applies appropriate configurations for each environment Docker Management: Handles image pulling, container management, and resource cleanup Health Monitoring: Verifies application status after deployment Logging \u0026amp; Notification: Provides detailed logging and deployment result notifications Rollback Mechanism: Enables rollback to previous versions if errors occur Content Creating Deployment Scripts for React.js Application Creating Deployment Scripts for Node.js Application Domain \u0026amp; DNS Configuration with Route 53 "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/3-containerization/3.3-deloy-scripts/3.3.3-domain-route53/","title":"Domain &amp; DNS Configuration with Route53","tags":[],"description":"","content":"Introduction After successfully deploying your website on AWS using services like EC2, the next crucial step is setting up a domain and configuring DNS so users can access your website through a professional domain name. Amazon Route53 is AWS\u0026rsquo;s reliable and powerful DNS service, making it easy to manage domains and direct traffic to your web resources.\nBenefits of Using Route53 Amazon Route53 offers several advantages for DNS and domain management:\nHigh Reliability: Built on AWS\u0026rsquo;s global infrastructure, Route53 ensures 99.99% uptime and high fault tolerance.\nOptimized Performance: With a distributed network of DNS servers worldwide, Route53 uses Latency-based Routing to automatically direct users to the nearest AWS location, reducing page load times.\nSeamless Integration: Works smoothly with other AWS services like EC2, S3, CloudFront, and Load Balancer, simplifying configuration.\nStrong Security: Supports DNSSEC (DNS Security Extensions) to protect against DNS spoofing attacks and integrates with AWS IAM for fine-grained access control.\nScalability: Automatically handles large DNS query volumes without manual management, ideal for websites with fluctuating traffic.\nCentralized Management: A unified interface in AWS Console allows you to manage all domains and DNS records in one place.\nImplementation Steps Option 1: Register a Domain Directly on AWS Route53\nStep 1: Go to Route 53 -\u0026gt; Register a domain Step 2: Enter your desired domain name and choose a suitable pricing plan Step 3: Select the domain you want and proceed to checkout\nStep 4: Verify successful payment as shown below Step 5: Go to Route 53 -\u0026gt; Create hosted zones\nStep 6: Enter the domain name you just registered Step 7: Successful hosted zone creation should look like this: Step 8: Select Create record\nRecord 1: Record name: \u0026ldquo;\u0026rdquo;, Value: IP EC2 Record 2: Record name: \u0026ldquo;www\u0026rdquo;, Value: IP EC2 Step 9: DNS propagation may take up to 2 hours.\nStep 10: Install Nginx on EC2 Instances with:\nsudo apt update \u0026amp;\u0026amp; sudo apt install nginx -y \u0026amp;\u0026amp; sudo systemctl start nginx \u0026amp;\u0026amp; sudo systemctl enable nginx Step 11: Create an Nginx configuration directory:\nsudo vim /etc/nginx/sites-available/mern-app Step 12: Add the following configuration: ngink config server { listen 80; server_name cheese1512.com; location /api/ { proxy_pass http://localhost:8080/; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } location / { proxy_pass http://localhost:5173/; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection 'upgrade'; proxy_set_header Host $host; proxy_cache_bypass $http_upgrade; } } **Step 13:**Install Certbot for SSL and enable the Nginx site:\nsudo apt install certbot python3-certbot-nginx -y sudo ln -s /etc/nginx/sites-available/mern-app /etc/nginx/sites-enabled/ Step 14: Test \u0026amp; reload Nginx:\nsudo nginx -t sudo systemctl reload nginx Step 15: Request SSL certificate \u0026amp; verify DNS:\nsudo certbot --nginx -d testing.integrationsninjas.com nslookup testing.integrationsninjas.com Step 16: Once completed, your domain should be accessible.\nOption 2: Using a Domain from Another Provider\nStep 1: Visit link\nStep 2: Select Domains ‚Üí Register a new domain Step 3: Choose a domain and complete payment. Step 4: After successful payment, repeat Steps 10-15 from Option 1.\nStep 5: Verify the results.\n"},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/3-containerization/3.1-development-env/3.1.3-clone-project/","title":"Instructions for cloning the FE&amp;BE project and uploading it to Github Repository","tags":[],"description":"","content":"After creating your two repositories on GitHub, you\u0026rsquo;ll need to download Visual Studio Code v√† GitHub Desktop These tools will help streamline cloning and pushing projects to GitHub\nWhat is Visual Studio Code Visual Studio Code (VS Code) is a free, cross-platform source code editor developed by Microsoft. It has quickly become one of the most popular editors among developers due to its user-friendly interface, powerful features, and extensibility.\nWhy Choose VS Code?\nFree \u0026amp; Open-Source ‚Äì Licensed under MIT, allowing free use, modification, and contributions Cross-Platform ‚Äì Works on Windows, macOS, and Linux. User-Friendly ‚Äì Simple, intuitive interface for beginners and experts. Multi-Language Support ‚Äì Supports JavaScript, Python, Java, C++, C#, Go, PHP, and more with IntelliSense (smart code suggestions), debugging, and syntax highlighting. What is GitHub Desktop ?\nGitHub Desktop is a GUI tool that simplifies Git workflows (cloning, committing, pushing) without using the command line.\nPros \u0026amp; Cons\nPros:\nEasy-to-use interface for managing repositories. Seamless GitHub integration (pull requests, code reviews). Auto-updates for security \u0026amp; features. Cons:\nNo official Linux support. Limited advanced Git features (e.g., interactive rebase). Optimized for GitHub (less support for other Git platforms). Steps to Clone FE/BE Projects \u0026amp; Push to GitHub Step 1: Access the FE \u0026amp; BE template repositories: FE Repository and BE repository. **Step 2:**Copy the BE repository URL, then open GitHub Desktop Step 3: In GitHub Desktop, go to: File -\u0026gt; Clone a repository -\u0026gt; URL. Step 4: Paste the repository URL and choose a local folder to save it. Step 5: After cloning, open VS Code. Step 6: Select File -\u0026gt; Open folder to start working -\u0026gt; Select the cloned project folder. Step 7: Open a terminal Terminal -\u0026gt; New Terminal and run: npm install. Step 8: Run the project: Step 9: Push the project to your own repository (AWS-BE) Step 10: Remove the old remote: git remote remove origin Step 11: Copy your AWS-BE repository URL from GitHub: Step 12: Add your new remote and push: git push -u origin main. Step 13: Repeat Steps 2-12 for the FE project. "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/4-securitymonitoring/4.3-aws-cloudwatch/","title":"Monitor and automatically notify with CloudWatch and SNS","tags":[],"description":"","content":"Introduction In today\u0026rsquo;s cloud environments, continuous monitoring and rapid incident response are critical factors determining application success. Amazon CloudWatch and Amazon Simple Notification Service (SNS) form a powerful monitoring and alerting system that enables 24/7 application performance tracking and instant notifications when anomalies occur.\nAmazon CloudWatch - The Eyes of AWS Infrastructure\nAmazon CloudWatch is a comprehensive monitoring and observability service that provides visibility into all your AWS resources and applications·∫°n Amazon SNS - Multi-Channel Notification System\nAmazon Simple Notification Service (SNS) is a fully managed messaging service that enables notifications to multiple endpoints With CloudWatch and SNS, you can build an intelligent monitoring system that automatically detects anomalies and ensures operations teams are promptly informed about system status. This forms the foundation for effective DevOps practices and reliable system operations.\nImplementation Steps Step 1: Navigate to AWS Console -\u0026gt; CloudWatch Step 2: Select Logs -\u0026gt; Log groups -\u0026gt; Create log group Step 3: Configure the following settings:\nLog group name: Mernstack-Cloudwatch Retention setting: 1 days Log class: Standard Step 4: Select Create Step 5: After successfully creating the Log group, navigate to AWS Console -\u0026gt; WAF \u0026amp; Shield\nStep 6: Select Mernstack-WAF -\u0026gt;Logging and metrics Step 7: Select Enable-\u0026gt; Logging destination Step 8: Select the Log Group you created, Redacted fields: HTTP Method, then select Save\nStep 9: Navigate to AWS Console -\u0026gt; Simple Notification Service -\u0026gt; Next step Step 10: Configure the following settings:\nType: Standard Name: Mernstack-SNS Advanced options:\nDelivery retry policy: Default Delivery status logging: Disabled (for lab) Encryption: Disabled (for lab) Access policy: Default Step 11: Select Create Topic and then verify the result Step 12: Select Create subscription Step 13: Select Protocol: Email, Endpoint: nguyenboo2018@gmail.com Step 14: Select Create Subscription Step 15: Navigate to AWS CloudWatch -\u0026gt; Alarm -\u0026gt; In Alarm -\u0026gt; Create alarm Step 16: Configure the following:\nSelect select metric -\u0026gt; EC2 -\u0026gt; Mernstack-EC2 Configure as shown below Set Alarm Name: Mernstack-Alarm then select Next -\u0026gt; Create Alarm "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/2-prerequiste/2.3-sgandnetwork/","title":"Security Groups &amp; Network Setup","tags":[],"description":"","content":"Content Create Security Groups Setup Network "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/3-containerization/","title":"Setup and Containerization","tags":[],"description":"","content":"Content Prepare Development Environment Create Docker Container Prepare Deployment Scripts "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/2-prerequiste/2.4-createiamrole/","title":"Create IAM Role","tags":[],"description":"","content":"Create IAM Role In this step, we will proceed to create IAM Role. In this IAM Role, the policy WebsiteKMSPolicy will be assigned, this is the policy that allows the EC2 server to communicate with the Session Manager.\nGo to IAM service administration interface In the left navigation bar, click Roles. Click Create role. Click AWS service and click EC2. Click Next: Permissions. In the Search box, enter AmazonSSMManagedInstanceCore and press Enter to search for this policy. Click the policy AmazonSSMManagedInstanceCore. Click Next: Tags. Click Next: Review. Name the Role SSM-Role in Role Name Click Create Role . Next, we will make the connection to the EC2 servers we created with Session Manager.\n"},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/4-securitymonitoring/","title":"Security &amp; Monitoring Application","tags":[],"description":"","content":"Introduction Trong ki·∫øn tr√∫c ·ª©ng d·ª•ng web hi·ªán ƒë·∫°i, vi·ªác ƒë·∫£m b·∫£o b·∫£o m·∫≠t v√† gi√°m s√°t h·ªá th·ªëng l√† y·∫øu t·ªë then ch·ªët quy·∫øt ƒë·ªãnh s·ª± th√†nh c√¥ng c·ªßa d·ª± √°n. Ph·∫ßn n√†y s·∫Ω h∆∞·ªõng d·∫´n b·∫°n tri·ªÉn khai c√°c bi·ªán ph√°p b·∫£o m·∫≠t to√†n di·ªán v√† h·ªá th·ªëng gi√°m s√°t hi·ªáu qu·∫£ cho ·ª©ng d·ª•ng ƒë∆∞·ª£c deploy tr√™n AWS, bao g·ªìm vi·ªác b·∫£o v·ªá d·ªØ li·ªáu, ki·ªÉm so√°t truy c·∫≠p, ph√≤ng ch·ªëng t·∫•n c√¥ng v√† theo d√µi hi·ªáu su·∫•t h·ªá th·ªëng theo th·ªùi gian th·ª±c.\nN·ªôi dung M√£ h√≥a d·ªØ li·ªáu v·ªõi KMS B·∫£o v·ªá ·ª©ng d·ª•ng web v·ªõi AWS WAF Monitor and automatically notify with CloudWatch and SNS "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/3-containerization/3.2-create-dockercontainer/3.2.4-github-action/","title":"Set up a CI/CD Pipeline for React.js and Nodejs with GitHub Actions","tags":[],"description":"","content":"Introduction CI/CD (Continuous Integration/Continuous Deployment) automates the building, testing, and deployment of applications. GitHub Actions provides a powerful platform to set up CI/CD pipelines for both Frontend (React.js) and Backend (Node.js) applications.\nFrontend CI/CD Pipeline\nCreate .github/workflows/cicd.yml file: yaml name: Deploy React Application on: push: branches: - master jobs: build: runs-on: ubuntu-latest steps: - name: Checkout source uses: action/checkout@v4 - name: Login to DockerHub run: docker login -u ${{ secrets.DOCKER_USERNAME }} -p ${{ secrets.DOCKER_PASSWORD }} - name: Build Docker image run: docker build -t thanhlep/reactjs-web --build-arg REACT_APP_NODE_ENV='production' --build-arg REACT_APP_SERVER_URL='${{ secrets.REACT_APP_SERVER_URL }}' . - name: Push Docker image run: docker push thanhlep/reactjs-web deploy: needs: build runs-on: ubuntu-latest steps: - name: Pull image from DockerHub run: docker pull thanhlep/reactjs-web:latest - name: Delete old container run: docker rm -f reactjs-web-container - name: Run Docker container run: docker run -d -p 5173:80 --name reactjs-web-container thanhlep/reactjs-web Backend CI/CD Pipeline\nCreate .github/workflows/cicd.yml file:\nyaml name: Deploy Node Application on: push: branches: - main jobs: build: runs-on: ubuntu-latest steps: - name: Checkout Source uses: actions/checkout@v4 - name: Login to docker hub run: docker login -u ${{ secrets.DOCKER_USERNAME }} -p ${{ secrets.DOCKER_PASSWORD }} - name: Build Docker Image run: docker build -t thanhlep/nodejs-web . - name: Publish Image to docker hub run: docker push thanhlep/nodejs-web:latest deploy: needs: build runs-on: self-hosted steps: - name: Pull image from docker hub run: docker pull thanhlep/nodejs-web:latest - name: Delete old container run: docker rm -f nodejs-web-container - name: Run Docker Container run: docker run -d -p 4000:4000 --name nodejs-web-container -e MONGO_PASSWORD='${{ secrets.MONGO_PASSWORD }}' thanhlep/nodejs-web "},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/5-cleanup/","title":"Clean up resources","tags":[],"description":"","content":"We will perform the following steps to delete all the resources created in this exercise in the correct order to avoid dependency errors.\nStep 1: Remove Application Load Balancer (ALB)\nNavigate to EC2 Management Console Select Load Balancers from the left menu Choose the created Application Load Balancer Click Actions ‚Üí Delete Type \u0026ldquo;confirm\u0026rdquo; and click Delete Delete Target Groups Go to Target Groups Select the target group created for ALB Click Actions ‚Üí Delete Confirm with Yes, delete Step 2: Terminate EC2 Instances\nIn the EC2 Dashboard Select Instances Choose all instances (both public and private) Click Instance State ‚Üí Terminate instance Confirm with Terminate Step 3: Delete other AWS Services\nDelete CloudWatch and SNS\nNavigate to CloudWatch Select Alarms ‚Üí choose created alarms ‚Üí Delete Go to Log groups ‚Üí select Log groups ‚Üí Delete log group(s) SNS Cleanup: Access SNS Console Select Topics ‚Üí choose topic ‚Üí Delete Go to Subscriptions ‚Üí select subscription ‚Üí Delete Route 53: Access Route 53 Console Select Hosted zones Delete record sets first, then delete the hosted zone Deleting KMS Keys Go to AWS KMS Console Select Customer managed keys Choose the key you created Click Key actions ‚Üí Schedule key deletion Set waiting period (minimum 7 days) Confirm with Schedule deletion WAF \u0026amp; Shield: Go to WAF \u0026amp; Shield Console Select Web ACLs Choose Web ACL ‚Üí Delete Type \u0026ldquo;delete\u0026rdquo; to confirm Step 5: Remove CloudFront Distribution\nAccess CloudFront Console Select the distribution Click Disable and wait for status to change to \u0026ldquo;Disabled\u0026rdquo; Then click Delete Step 6: X√≥a IAM Roles v√† Users\nNavigate to IAM Console For Roles: Select created roles (SSM-Role, ECS-Role, etc.) Click Delete Enter role name to confirm For Policies: Filter by \u0026ldquo;Customer managed\u0026rdquo; Select policy ‚Üí Actions ‚Üí Delete Step 7: Remove Storage Resources\nS3 Buckets:\nGo to S3 Console Select the bucket Click Empty Type \u0026ldquo;permanently delete\u0026rdquo; ‚Üí Empty Then click Delete bucket Enter bucket name to confirm Step 8: Remove Network Gateways\nNAT Gateway: Select NAT Gateways Choose NAT Gateway ‚Üí Actions ‚Üí Delete NAT gateway Type \u0026ldquo;delete\u0026rdquo; to confirm Internet Gateway: Access Internet Gateways Select IGW ‚Üí Actions ‚Üí Detach from VPC Then Actions ‚Üí Delete internet gateway Step 9: Final VPC Cleanup\nDelete Security Groups (except default): Select created security groups -Actions ‚Üí Delete security group Remove Subnets: Select all created subnets Actions ‚Üí Delete subnet Delete VPC: Select the Lab VPC Actions ‚Üí Delete VPC Type \u0026ldquo;delete\u0026rdquo; to confirm Important Notes ‚ö†Ô∏è Order Matters: Always delete dependent resources first\n‚ö†Ô∏è Region Check: Verify you\u0026rsquo;re working in the correct AWS Region.\n‚ö†Ô∏è CloudFront: Disable before deleting (takes 15-20 minutes)\n‚ö†Ô∏è Billing: Check AWS Billing Console to confirm no active resources\n‚úÖ Completion: After completing all steps, your AWS environment should be completely clean with no remaining resources from the lab. This ensures you won\u0026rsquo;t incur any unexpected charges.\n"},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://thanhlong1512.github.io/Workshop-AWS/tags/","title":"Tags","tags":[],"description":"","content":""}]